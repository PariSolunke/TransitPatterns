{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901ada9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/21 04:18:54 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/04/21 04:18:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/04/21 04:18:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/04/21 04:18:54 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "cf = SparkConf()\n",
    "cf.set(\"spark.submit.deployMode\",\"client\")\n",
    "sc = SparkContext.getOrCreate(cf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "\t    .builder \\\n",
    "\t    .appName(\"Python Spark SQL basic example\") \\\n",
    "\t    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t    .getOrCreate()\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8143b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, TimestampType, DateType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "# Load the data into a Spark DataFrame\n",
    "data = spark.read.csv(path='/shared/TransitTrends/MTA_RAW', header=True, inferSchema=True)\n",
    "\n",
    "#Load the reference data for complex id's for stations\n",
    "rcl=spark.read.csv('/shared/TransitTrends/rcl.csv' ,header=True)\n",
    "\n",
    "# Clean the column names\n",
    "data = data.select([F.col(column).alias(column.strip()) for column in data.columns])\n",
    "\n",
    "# Combine DATE and TIME into a single column, and convert to timestamp\n",
    "data = data.withColumn(\"DATE_TIME\", F.to_timestamp(F.concat(\"DATE\", \"TIME\"), \"MM/dd/yyyyHH:mm:ss\"))\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.dropDuplicates([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(\"DESC\", \"LINENAME\", \"DIVISION\")\n",
    "\n",
    "# Aggregate ENTRIES and EXITS by taking the maximum value for each group\n",
    "\n",
    "w = Window.partitionBy(\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE\")\n",
    "data = data.withColumn('LastTimes', F.max('DATE_TIME').over(w))\\\n",
    "    .where(F.col('DATE_TIME') == F.col('LastTimes'))\\\n",
    "    .drop('LastTimes')\n",
    "\n",
    "# Merge with rcl DataFrame and drop duplicates\n",
    "data = data.join(rcl.select(\"complex_id\", \"remote\"), F.col(\"UNIT\") == F.col(\"remote\"), \"left\").drop(\"remote\").dropDuplicates()\n",
    "\n",
    "#Rename complex_id\n",
    "data = data.withColumnRenamed(\"complex_id\", \"COMPLEX_ID\")\n",
    "\n",
    "#Convert DATE_TIME to DATE\n",
    "data = data.withColumn(\"DATE\", F.to_date(F.col(\"DATE_TIME\")))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(\"TIME\", \"DATE_TIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929299cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Prev_Entries and Prev_Exits columns to dataframe based on previous days entries and exits\n",
    "w = Window.partitionBy(\"C/A\", \"UNIT\", \"SCP\", \"STATION\").orderBy(\"DATE\")\n",
    "data = data.withColumn(\"PREV_DATE\", F.lag(F.col(\"DATE\"), 1).over(w))\n",
    "data = data.withColumn(\"PREV_ENTRIES\", F.lag(F.col(\"ENTRIES\"), 1).over(w))\n",
    "data = data.withColumn(\"PREV_EXITS\", F.lag(F.col(\"EXITS\"), 1).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280e2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fi;ter for data in 2019-2022, removing the last week of 2018 from the dataset\n",
    "data = data.filter(F.year(\"DATE\").isin([2019, 2020, 2021, 2022]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2f5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the daily Entries and Exits for each turnstile and setting limits to ensure bad values are filtered out\n",
    "data = data.withColumn(\"DAILY_ENTRIES\", F.col(\"ENTRIES\") - F.col(\"PREV_ENTRIES\"))\n",
    "data = data.withColumn(\"DAILY_EXITS\", F.col(\"EXITS\") - F.col(\"PREV_EXITS\"))\n",
    "\n",
    "data = data.withColumn(\"DAILY_ENTRIES\", F.when(F.col(\"DAILY_ENTRIES\") < 0, 0)\n",
    "                       .when(F.col(\"DAILY_ENTRIES\") > 100000, 100000)\n",
    "                       .otherwise(F.col(\"DAILY_ENTRIES\")))\n",
    "data = data.withColumn(\"DAILY_EXITS\", F.when(F.col(\"DAILY_EXITS\") < 0, 0)\n",
    "                       .when(F.col(\"DAILY_EXITS\") > 100000, 100000)\n",
    "                       .otherwise(F.col(\"DAILY_EXITS\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03d32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agg_data = data.groupBy(\"STATION\", \"COMPLEX_ID\", \"DATE\").agg(F.sum(\"DAILY_ENTRIES\").alias(\"ENTRIES\"), F.sum(\"DAILY_EXITS\").alias(\"EXITS\")).orderBy(\"STATION\",\"COMPLEX_ID\", \"DATE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee7c8283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+-----+\n",
      "|STATION|COMPLEX_ID|      DATE|ENTRIES|EXITS|\n",
      "+-------+----------+----------+-------+-----+\n",
      "|   1 AV|     119.0|2019-01-01|   9989|12097|\n",
      "|   1 AV|     119.0|2019-01-02|  18476|21058|\n",
      "|   1 AV|     119.0|2019-01-03|  19866|22531|\n",
      "|   1 AV|     119.0|2019-01-04|  20389|23633|\n",
      "|   1 AV|     119.0|2019-01-05|  13930|16661|\n",
      "|   1 AV|     119.0|2019-01-06|  12021|13789|\n",
      "|   1 AV|     119.0|2019-01-07|  19538|22308|\n",
      "|   1 AV|     119.0|2019-01-08|  20414|22397|\n",
      "|   1 AV|     119.0|2019-01-09|  21066|23373|\n",
      "|   1 AV|     119.0|2019-01-10|  21178|23377|\n",
      "|   1 AV|     119.0|2019-01-11|  21612|24818|\n",
      "|   1 AV|     119.0|2019-01-12|  15137|18197|\n",
      "|   1 AV|     119.0|2019-01-13|  12232|14431|\n",
      "|   1 AV|     119.0|2019-01-14|  19474|22219|\n",
      "|   1 AV|     119.0|2019-01-15|  20349|22619|\n",
      "|   1 AV|     119.0|2019-01-16|  20866|23107|\n",
      "|   1 AV|     119.0|2019-01-17|  21631|23915|\n",
      "|   1 AV|     119.0|2019-01-18|  21097|24298|\n",
      "|   1 AV|     119.0|2019-01-19|  14953|17511|\n",
      "|   1 AV|     119.0|2019-01-20|  11336|13311|\n",
      "+-------+----------+----------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agg_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b276bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://nyu-dataproc-m/shared/TransitTrends/MTA_CLEAN/daily_counts.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Write CSV\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43magg_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/shared/TransitTrends/MTA_CLEAN/daily_counts.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1372\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   1366\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m   1367\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m   1371\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m-> 1372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://nyu-dataproc-m/shared/TransitTrends/MTA_CLEAN/daily_counts.csv already exists."
     ]
    }
   ],
   "source": [
    "#Write CSV\n",
    "agg_data.write.csv(\"/shared/TransitTrends/MTA_CLEAN/daily_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f8096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
