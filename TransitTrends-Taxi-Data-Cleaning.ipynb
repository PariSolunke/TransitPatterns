{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f297664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/22 01:00:21 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/04/22 01:00:21 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/04/22 01:00:21 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/04/22 01:00:21 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "cf = SparkConf()\n",
    "cf.set(\"spark.submit.deployMode\",\"client\")\n",
    "sc = SparkContext.getOrCreate(cf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "\t    .builder \\\n",
    "\t    .appName(\"TransitTrends- Taxi - Data Cleaning\") \\\n",
    "\t    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd9b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                         (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1211, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:39067)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_739/2157012696.py\", line 5, in <cell line: 5>\n",
      "    green_cabs_df = spark.read.parquet(\"/shared/TAXI/GREEN_CABS/\").withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 458, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o77.parquet\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/executing/executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object run_code at 0x7f3557baebe0, file \"/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3333>, 139867081862112, 74)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:39067)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_739/2157012696.py\", line 5, in <cell line: 5>\n",
      "    green_cabs_df = spark.read.parquet(\"/shared/TAXI/GREEN_CABS/\").withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 458, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o77.parquet\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/executing/executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object run_code at 0x7f3557baebe0, file \"/opt/conda/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3333>, 139867081862112, 74)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o77.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lit\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# importing green cabs data, dropping unnecessary coloumns and renaming few coloumns to match yellow cabs data:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#green_cabs_df = spark.read.csv(path='/shared/TAXI_SAMPLE/GREEN_CABS/',header=True, inferSchema= True).withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m green_cabs_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/shared/TAXI/GREEN_CABS/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaxi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# importing yellow cabs data:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#yellow_cabs_df = spark.read.csv(path='/shared/TAXI_SAMPLE/YELLOW_CABS/',header=True, inferSchema= True).withColumn(\"taxi_type\", lit(\"yellow\"))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m yellow_cabs_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/shared/TAXI/YELLOW_CABS/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaxi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:458\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    453\u001b[0m recursiveFileLookup \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursiveFileLookup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m    455\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[1;32m    456\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter)\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o77.parquet"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# importing green cabs data, dropping unnecessary coloumns and renaming few coloumns to match yellow cabs data:\n",
    "#green_cabs_df = spark.read.csv(path='/shared/TAXI_SAMPLE/GREEN_CABS/',header=True, inferSchema= True).withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n",
    "green_cabs_df = spark.read.parquet(\"/shared/TAXI/GREEN_CABS/\").withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n",
    "\n",
    "# importing yellow cabs data:\n",
    "#yellow_cabs_df = spark.read.csv(path='/shared/TAXI_SAMPLE/YELLOW_CABS/',header=True, inferSchema= True).withColumn(\"taxi_type\", lit(\"yellow\"))\n",
    "yellow_cabs_df = spark.read.parquet(\"/shared/TAXI/YELLOW_CABS/\").withColumn(\"taxi_type\", lit(\"yellow\"))\n",
    "\n",
    "# note: sometime there might be .ipynb checkpoints in /shared/GREEN_CABS folder. remove them hdfs before running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(green_cabs_df.count())\n",
    "green_cabs_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1744e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yellow_cabs_df.count())\n",
    "yellow_cabs_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining yellow and green cabs data into a single dataset:\n",
    "cabs_df = yellow_cabs_df.union(green_cabs_df)\n",
    "# dropping unnnecessary coloumns:\n",
    "cabs_df = cabs_df.drop(\"store_and_fwd_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cabs_df.count())\n",
    "cabs_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the column names\n",
    "cabs_df = cabs_df.select([F.col(column).alias(column.strip()) for column in cabs_df.columns])\n",
    "cabs_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "cabs_df = cabs_df.dropDuplicates([\"taxi_type\",\"total_amount\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"pulocationid\",\"dolocationid\", \"passenger_count\", \"vendorid\", \"trip_distance\"])\n",
    "print(cabs_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for data in 2019-2021, removing the last week of 2018 from the dataset\n",
    "cabs_df = cabs_df.filter(F.year(\"tpep_pickup_datetime\").isin([2019, 2020, 2021]))\n",
    "print(cabs_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "cabs_df = cabs_df.withColumn(\"pickup_datetime\", cabs_df[\"tpep_pickup_datetime\"].cast(\"timestamp\"))\n",
    "cabs_df = cabs_df.withColumn(\"month\", F.month(cabs_df[\"pickup_datetime\"])).withColumn(\"year\", F.year(cabs_df[\"pickup_datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ed3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_records = cabs_df.select(\"month\",\"year\")\n",
    "#cabs_df = monthly_records.withColumn(\"month/year\", concat(F.col(\"month\"), F.lit(\"/\"), F.col(\"year\"))).drop(\"month\",\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_records.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a83a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,asc\n",
    "\n",
    "count_df = monthly_records.groupBy('year','month').agg(count('*').alias('count')).orderBy(asc('year'), asc('month'))\n",
    "count_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac21e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the PySpark DataFrame to Pandas DataFrame\n",
    "pd = count_df.toPandas()\n",
    "\n",
    "# Create the bar plot using matplotlib\n",
    "plt.bar(pd['year'].astype(str) + '-' + pd['month'].astype(str), pd['count'])\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('Trips per Year-Month')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabs_monthly_trips_df = cabs_df.groupBy('month/year').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "cabs_monthly_trips_df = cabs_monthly_trips_df.orderBy(asc('month/year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabs_monthly_trips_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabs_monthly_trips_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b56b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fddd13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = cabs_monthly_trips_df.toPandas()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pandas_df['month/year'], pandas_df['count'])\n",
    "plt.xlabel('Month/Year')\n",
    "plt.ylabel('Trip Count')\n",
    "plt.title('Monthly Cab Trips')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ae692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
