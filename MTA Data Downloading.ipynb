{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b842209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a911aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all of the links from this webpage and save them in a list\n",
    "url=\"http://web.mta.info/developers/turnstile.html\"\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    " \n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b476b95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Filter the list to only show links pointing to turnstile data for years 2019-2022\n",
    "\n",
    "start19='data/nyct/turnstile/turnstile_19'\n",
    "start20='data/nyct/turnstile/turnstile_20'\n",
    "start21='data/nyct/turnstile/turnstile_21'\n",
    "start22='data/nyct/turnstile/turnstile_22'\n",
    "data_links=[]\n",
    "for link in links:\n",
    "    if link:\n",
    "        if link.startswith(start19) or link.startswith(start20) or link.startswith(start21) or link.startswith(start22):\n",
    "            data_links.append('http://web.mta.info/developers/'+link)\n",
    "            \n",
    "data_links.append('http://web.mta.info/developers/data/nyct/turnstile/turnstile_181229.txt')\n",
    "data_links.reverse()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f75d7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the links to a temporary pandas dataframe and save these as csv files\n",
    "for link in data_links:\n",
    "    print(counter)\n",
    "    data=pd.read_csv(link)\n",
    "    data.to_csv(\"raw_csv_mta/\"+str(counter)+\".csv\", index=False)\n",
    "\n",
    "#File to lookup complex_number for each station    \n",
    "rcl=pd.read_csv(\"https://raw.githubusercontent.com/qri-io/data-stories-scripts/master/nyc-turnstile-counts/lookup/remote_complex_lookup.csv\", index_col=False)\n",
    "rcl.columns = [column.strip() for column in rcl.columns]\n",
    "rcl.to_csv(\"rcl.csv\", index=False)\n",
    "\n",
    "##This downloads all the chosen datafiles, which we then put to HDFS by running the put command in dataproc terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90532fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cleaned_data=pd.DataFrame()\\ncounter = 1\\nfor link in data_links:\\n    print(\"Processing Data from Link Number:\",counter)\\n    data=pd.read_csv(link)\\n    data.columns = [column.strip() for column in data.columns]\\n    data[\"DATE_TIME\"] = pd.to_datetime(data.DATE + \" \" + data.TIME, format=\"%m/%d/%Y %H:%M:%S\")\\n    data.drop_duplicates(subset=[\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"], inplace=True)\\n    data = data.drop([\"DESC\",\"LINENAME\",\"DIVISION\"], axis=1, errors=\"ignore\")\\n    data = data.groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE\"]).last().reset_index()\\n    data=data.merge(rcl[[\\'complex_id\\', \\'remote\\']], how = \\'left\\', left_on = \\'UNIT\\', right_on = \\'remote\\').drop_duplicates().reset_index(drop=True)\\n    data[\\'DATE\\'] = pd.to_datetime(data[\\'DATE_TIME\\']).dt.date\\n    data = data.drop([\"remote\", \"TIME\", \"DATE_TIME\"], axis=1, errors=\"ignore\")\\n    cleaned_data= cleaned_data.append(data)\\n    counter+=1\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pandas only cleaning, this is not being used anymore\n",
    "\"\"\"cleaned_data=pd.DataFrame()\n",
    "counter = 1\n",
    "for link in data_links:\n",
    "    print(\"Processing Data from Link Number:\",counter)\n",
    "    data=pd.read_csv(link)\n",
    "    data.columns = [column.strip() for column in data.columns]\n",
    "    data[\"DATE_TIME\"] = pd.to_datetime(data.DATE + \" \" + data.TIME, format=\"%m/%d/%Y %H:%M:%S\")\n",
    "    data.drop_duplicates(subset=[\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"], inplace=True)\n",
    "    data = data.drop([\"DESC\",\"LINENAME\",\"DIVISION\"], axis=1, errors=\"ignore\")\n",
    "    data = data.groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE\"]).last().reset_index()\n",
    "    data=data.merge(rcl[['complex_id', 'remote']], how = 'left', left_on = 'UNIT', right_on = 'remote').drop_duplicates().reset_index(drop=True)\n",
    "    data['DATE'] = pd.to_datetime(data['DATE_TIME']).dt.date\n",
    "    data = data.drop([\"remote\", \"TIME\", \"DATE_TIME\"], axis=1, errors=\"ignore\")\n",
    "    cleaned_data= cleaned_data.append(data)\n",
    "    counter+=1\n",
    "\n",
    "cleaned_data[[\"PREV_DATE\", \"PREV_ENTRIES\", \"PREV_EXITS\"]] = (cleaned_data.groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\"])[\"DATE\", \"ENTRIES\", \"EXITS\"].transform(lambda grp: grp.shift(1)))\n",
    "cleaned_data.to_csv(\"intermediate_cleaned_mta.csv\",index=False)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
