{
  "metadata": {
    "name": "TransitTrends - Taxi - Data Cleaning",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom pyspark import SparkContext, SparkConf\ncf \u003d SparkConf()\ncf.set(\"spark.submit.deployMode\",\"client\")\nsc \u003d SparkContext.getOrCreate(cf)\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession \\\n\t    .builder \\\n\t    .appName(\"TransitTrends- Taxi - Data Cleaning\") \\\n\t    .config(\"spark.some.config.option\", \"some-value\") \\\n\t    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom pyspark.sql.functions import lit\n\n# importing green cabs data, dropping unnecessary coloumns and renaming few coloumns to match yellow cabs data:\n#green_cabs_df \u003d spark.read.csv(path\u003d\u0027/shared/TAXI_SAMPLE/GREEN_CABS/\u0027,header\u003dTrue, inferSchema\u003d True).withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\ngreen_cabs_df \u003d spark.read.parquet(\"/shared/TAXI/GREEN_CABS/\").withColumn(\"taxi_type\", lit(\"green\")).drop(\"trip_type\").withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\").withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n\n# importing yellow cabs data:\n#yellow_cabs_df \u003d spark.read.csv(path\u003d\u0027/shared/TAXI_SAMPLE/YELLOW_CABS/\u0027,header\u003dTrue, inferSchema\u003d True).withColumn(\"taxi_type\", lit(\"yellow\"))\nyellow_cabs_df \u003d spark.read.parquet(\"/shared/TAXI/YELLOW_CABS/\").withColumn(\"taxi_type\", lit(\"yellow\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(green_cabs_df.count())\ngreen_cabs_df.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(yellow_cabs_df.count())\nyellow_cabs_df.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# combining yellow and green cabs data into a single dataset:\ncabs_df \u003d yellow_cabs_df.union(green_cabs_df)\n# dropping unnnecessary coloumns:\ncabs_df \u003d cabs_df.drop(\"store_and_fwd_flag\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(cabs_df.count())\ncabs_df.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport pyspark.sql.functions as F"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Clean the column names\ncabs_df \u003d cabs_df.select([F.col(column).alias(column.strip()) for column in cabs_df.columns])\ncabs_df.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Remove duplicates\ncabs_df \u003d cabs_df.dropDuplicates([\"taxi_type\",\"total_amount\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"pulocationid\",\"dolocationid\", \"passenger_count\", \"vendorid\", \"trip_distance\"])\ncabs_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#Filter for data in 2019-2021, removing the last week of 2018 from the dataset\ncabs_df \u003d cabs_df.filter(F.year(\"tpep_pickup_datetime\").isin([2019, 2020, 2021]))\ncabs_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom pyspark.sql.functions import concat\ncabs_df \u003d cabs_df.withColumn(\"pickup_datetime\", cabs_df[\"tpep_pickup_datetime\"].cast(\"timestamp\"))\ncabs_df \u003d cabs_df.withColumn(\"month\", F.month(cabs_df[\"pickup_datetime\"])).withColumn(\"year\", F.year(cabs_df[\"pickup_datetime\"]))\ncabs_df \u003d cabs_df.withColumn(\"month/year\", concat(F.col(\"month\"), F.lit(\"/\"), F.col(\"year\"))).drop(\"month\",\"year\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ncabs_df.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ncabs_monthly_trips_df \u003d cabs_df.groupBy(\u0027month/year\u0027).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ncabs_monthly_trips_df.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ncabs_monthly_trips_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\n\n# convert month/year string to unix timestamp\ndf \u003d df.withColumn(\u0027timestamp\u0027, unix_timestamp(\u0027month/year\u0027, \u0027MM/yyyy\u0027))\n\n# convert unix timestamp to date string\ndf \u003d df.withColumn(\u0027date\u0027, from_unixtime(\u0027timestamp\u0027, \u0027yyyy-MM-dd\u0027))\n\n# sort by timestamp\ndf \u003d df.sort(\u0027timestamp\u0027)\n\n# create x and y axis data\nx \u003d df.select(\u0027date\u0027).rdd.flatMap(lambda x: x).collect()\ny \u003d df.select(\u0027count\u0027).rdd.flatMap(lambda x: x).collect()\n\n# plot bar graph\nplt.bar(x, y)\nplt.xticks(rotation\u003d45)\nplt.xlabel(\u0027Month/Year\u0027)\nplt.ylabel(\u0027Count\u0027)\nplt.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}